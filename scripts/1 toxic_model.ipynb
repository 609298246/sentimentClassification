{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 几个变化\n",
    "- 在 dense 层前面加了 batch-norm 层\n",
    "- batch 大小为 2048\n",
    "- 让 embedding 层可训练\n",
    "\n",
    "# 效果\n",
    "- 加入 batch-norm 层之后，训练速度大大提高。\n",
    "- embedding 层可训练之后，AUC 不断提升。原来 embedding 层不可训练的时候，AUC 在第三个 epoch 的时候就停止增加了。\n",
    "\n",
    "\n",
    "# 需要调整的参数\n",
    "- batch-size\n",
    "- split 比例\n",
    "- drop rate. 由于测试集数据和训练集数据差不多大，因此，我们不能太依赖于网络结构，我们需要将网络设置得更加具有普遍适用性。所以这里将 dropout rate 设置稍微大些。范围在 0.6 到 0.1 之间。\n",
    "\n",
    "# 策略\n",
    "- step 1: 现有参数\n",
    "    - two LSTM layers both with 128 hidden units\n",
    "    - one dense layer doing classification\n",
    "    - batch-norm before each layer\n",
    "    - dropout after each layer and the dropout rate is 0.1\n",
    "    - epochs is 13\n",
    "    - batch-size is 2048\n",
    "- step 2: 接下来要做的事情\n",
    "    - observe auc, stop when it reaches 0.999\n",
    "    - use the epoch in which auc is 0.999 to train all models\n",
    "    - save models\n",
    "    - evaluate\n",
    "    - predict\n",
    "    - save the results and combine them together\n",
    "    - submit the results\n",
    "    - change dropout rate to 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "datafile = open('./data.pkl', 'rb')\n",
    "# text of train\n",
    "X_train = pickle.load(datafile)\n",
    "# text of test\n",
    "X_test = pickle.load(datafile)\n",
    "word_to_index = pickle.load(datafile)\n",
    "index_to_word = pickle.load(datafile)\n",
    "word_to_vec_map = pickle.load(datafile)\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二元分类的 AUC 的计算方式\n",
    "def auc(y_true, y_pred):\n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    N = K.sum(1 - y_true)\n",
    "    FP = K.sum(y_pred - y_pred * y_true)\n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    P = K.sum(y_true)\n",
    "    TP = K.sum(y_pred * y_true)\n",
    "    return TP/P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建预训练的 embedding 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用的预训练的 word embedding 是 40 万个单词的训练结果，它们的特征维数是 50\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    创建一个 Keras 的 Embedding() 层，并且加载之前已经训练好的 embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    # 词典中单词的个数+1，+1是 keras 模型的训练要求\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    # 获取单词的特征维数，随便找个单词就行了\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    \n",
    "    # 将 embedding 矩阵初始化为全 0 的，大小为 (vocab_len, emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # 将 emb_matrix 的行号当做单词的编号，然后将这个单词的 embedding 放到这一行，这样就把预训练的 embedding 加载进来了\n",
    "    # 注意，由于单词编号是从 1 开始的，所以行 0 是没有 embedding 的，这就是为什么前面要 +1\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # 创建 Keras 的Embedding 层\n",
    "    embedding_layer = Embedding(input_dim=vocab_len, output_dim=emb_dim, trainable=True)\n",
    "\n",
    "    # build embedding layer，在设置 embedding layer 的权重的时候，这一步是必须的\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # 将 emb_matrix 设置为 embedding_layer 的权重。\n",
    "    # 到这里为止我们就创建了一个预训练好的 embedding layer\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其他所有的分类模型可以基于这个函数进行创建\n",
    "def mother_model(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    返回：一个 Keras 的模型\n",
    "    \n",
    "    参数:\n",
    "    input_shape -- MAX_COMMENT_TEXT_SEQ\n",
    "    word_to_vec_map\n",
    "    word_to_index\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # 创建输入层，输入的是句子的单词编号列表\n",
    "    sentence_indices = Input(shape=input_shape, dtype=np.int32)\n",
    "    # 创建 word embedding 层\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    # 句子编号列表进入 embedding_layer 之后会返回对应的 embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    dr_r = 0.5\n",
    "    \n",
    "    X = BatchNormalization()(embeddings)\n",
    "    X = LSTM(128, return_sequences=True)(X)\n",
    "    X = Dropout(dr_r)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X, _, __ = LSTM(128, return_state = True)(X)\n",
    "    X = Dropout(dr_r)(X)\n",
    "    \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(64, activation='relu')(X)\n",
    "    X = Dropout(dr_r)(X)\n",
    "    \n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建 toxic 分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_COMMENT_TEXT_SEQ = 200\n",
    "toxic_model = mother_model((MAX_COMMENT_TEXT_SEQ,), word_to_vec_map, word_to_index)\n",
    "toxic_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 训练模型、评估模型、保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 148401 samples, validate on 11170 samples\n",
      "Epoch 1/50\n",
      "148401/148401 [==============================] - 2084s 14ms/step - loss: 0.5168 - auc: 0.8015 - val_loss: 0.2593 - val_auc: 0.9207\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25927, saving model to ./models/model-01.h5\n",
      "Epoch 2/50\n",
      "148401/148401 [==============================] - 2175s 15ms/step - loss: 0.2124 - auc: 0.9051 - val_loss: 0.1715 - val_auc: 0.9584\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25927 to 0.17151, saving model to ./models/model-02.h5\n",
      "Epoch 3/50\n"
     ]
    }
   ],
   "source": [
    "model_dir = './models'\n",
    "filepath = model_dir + '/model-{epoch:02d}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',save_best_only=True, verbose=1)\n",
    "callbacks_list = [checkpoint]\n",
    "train_result = toxic_model.fit(X_train['comment_text'], train[['toxic']], \n",
    "                    epochs=50, \n",
    "                    batch_size=2048, \n",
    "                    validation_split=0.07, \n",
    "                    callbacks = callbacks_list,\n",
    "                    verbose=1)\n",
    "\n",
    "plt.plot(train_result.history['train_loss'])\n",
    "plt.plot(train_result.history['validation_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
